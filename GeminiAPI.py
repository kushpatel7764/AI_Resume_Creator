import random
import json
import requests
from api_secrets import gemini_api_key
import sqlite3
# Name: Kush Patel
# Project Description: This program generates a markdown resume by combining a personal description with
#                      a randomly selected job description from rapid_jobs2.json. The markdown resume is
#                      generated by prompting the Gemini AI model to create a markdown resume based on this information.

#TODO: python -m pytest tests/*

# Function to get a random job listing from the JSON file
def get_random_json_object():
    with open('rapid_jobs2.json', 'r') as f:
        json_objects = []
        for line in f:
            # Read each line in rapid_jobs2.json file as a JSON object
            json_objects.append(json.loads(line))
        # Select a random section from the JSON list
        random_job_section = random.choice(json_objects)
        # Select a random job within the chosen section
        random_job = random.choice(random_job_section)
    return random_job

#Function to get all json objects
def get_all_json_objects():
    json_objects_job1 = [] #Single JSON object is each index
    json_objects_job2 = [] #Array of JSON objects is in each index
    json_objects_job2_objs_only = []
    with open('rapid_jobs2.json', 'r') as f:
        for line in f:
            # Read each line in rapid_jobs2.json file as a JSON object
            json_objects_job2.append(json.loads(line))
        for obj in json_objects_job2:
            json_objects_job2_objs_only.append(obj)

    with open('rapid_job1.json', 'r') as f:
        for line in f:
            # Read each line in rapid_jobs2.json file as a JSON object
            json_objects_job1.append(json.loads(line))
    with open('output.txt', 'w') as f:
        f.write(json.dumps(json_objects_job2_objs_only, indent=4))
    return json_objects_job1, json_objects_job2_objs_only


#Connect to database
def connect_job_database(cursor, conn):
    # Read the SQL file
    with open("job_database.sql", "r") as file:
        sql_script = file.read()

    # Execute the SQL script (AI helped find the executescript() function)
    cursor.executescript(sql_script)
    # Commit changes
    conn.commit()


# Store job_json into database
def insert_to_database(cursor, conn, job_info, company):
    cursor.execute("""
        Insert INTO jobs (id, site, job_url, job_url_direct, title, company_id, location, job_type, date_posted, salary_source, interval, min_amount, max_amount, currency, is_remote, job_level, job_function, listing_type, emails, description, employment_type, salary_range, image, provider_name, provider_url) 
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? ,?, ?, ?, ?, ?, ?, ?, ?);
    """, (job_info.get("id"),
        job_info.get("site"),
        job_info.get("job_url"),
        job_info.get("job_url_direct"),
        job_info.get("title"),
        company.get("id"),  # Use company ID as foreign key
        job_info.get("location"),
        job_info.get("job_type"),
        job_info.get("date_posted"),
        job_info.get("salary_source"),
        job_info.get("interval"),
        job_info.get("min_amount"),
        job_info.get("max_amount"),
        job_info.get("currency"),
        job_info.get("is_remote"),
        job_info.get("job_level"),
        job_info.get("job_function"),
        job_info.get("listing_type"),
        job_info.get("emails"),
        job_info.get("description"),
        job_info.get("employment_type"),
        job_info.get("salary_range"),
        job_info.get("image"),
        job_info.get("provider_name"),
        job_info.get("provider_url"))
    )
    conn.commit()

    cursor.execute("""
        Insert INTO companies (id, name, company_industry, company_url, company_url_direct, company_addresses, company_num_employees, company_revenue, company_description, logo_photo_url, banner_photo_url, ceo_name, ceo_photo_url) 
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
    """, (job_info.get("id"),
          job_info.get("name"),
          job_info.get("company_industry"),
          job_info.get("company_url"),
          job_info.get("company_url_direct"),
          company.get("company_addresses"),  # Use company ID as foreign key
          job_info.get("company_num_employees"),
          job_info.get("company_revenue"),
          job_info.get("company_description"),
          job_info.get("logo_photo_url"),
          job_info.get("banner_photo_url"),
          job_info.get("ceo_name"),
          job_info.get("ceo_photo_url"))
    )
    conn.commit()



def main():
    # Get the API key from the api_secrets file to access Google's Gemini AI model
    key = gemini_api_key

    # Using the random_json_object(), get a random job and then extract the job description from the job.
    job_json_random = get_random_json_object()
    job_description = job_json_random["description"]



    # Connect to the SQLite database (or create one if it doesn't exist)
    conn = sqlite3.connect("Jobs_Database.db")
    cursor = conn.cursor()
    #Connect the Jobs_Database.db
    connect_job_database(cursor, conn)
    #Insert json info in database
    insert_to_database(cursor, conn, job_json_random, job_json_random["company"])


    # Personal information to be included in the resume
    personal_info = ("My name is Kush Patel. I am a computer science major studying at Bridgewater State University (BSU),"
                     " I am driven by a desire to innovate and problem solve. I am graduating from the BSU on May, 2025. "
                     "Some important courses I have completed at BSU are Web Application Development, Computer Networks, "
                     "Software Engineering, Cloud Computing, Introduction Database systems, Introduction to A.I., and "
                     "Unix/Linux System Admin.I was awarded Dr. Linda Wilkens and Dr. Glenn Pavlicek Scholarship at BSU in "
                     "recognition of my academic achievements such as a 4.0 major GPA. I am from East Greenwich, Road Island "
                     "but I currently live in Fairhaven, Massachusetts. Some programing languages that I am proficient at are"
                     " Python, Java, Swift, and JavaScript. I also MySQL from my database system course. Currently, I am "
                     "developing a bank statement processing application that efficiently converts bank statements into Excel"
                     " spreadsheets. With a positive attitude and a relentless motivation to learn, I'm eager to take on new "
                     "challenges and expand my expertise. Additionally, along with my current project, I am also doing a "
                     "research internship at Bridgewater State University in which I am developing a program that can recognize"
                     " a table in an image and convert it to an excel table. Some tools that I know are Git, JetBrain IDEs, "
                     "Xcode, and Vscode. My hobbies are playing basketball, cricket, and programing.")

    # Prepare the API request payload for Google's Gemini AI model
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={key}"
    headers = {'content-type': 'application/json'}
    data = {
        "contents": [{
            "parts": [{"text": f"Remember my Personal Information: {personal_info} \n Remember the Job description: "
                               f"{job_description} \n Now create a resume in markdown format that will be designed from "
                               f"my personal information, using keywords from job description that I provided"}]
        }]
    }

    # Send a post request to the Gemini API
    response = requests.post(url, headers=headers, json=data)
    response_json = response.json()

    # Extract the generated markdown resume from the API response
    marked_resume = response_json["candidates"][0]["content"]["parts"][0]["text"]

    # Save the generated resume to a markdown file
    new_file_name = "Marked_Resume.md"
    with open(new_file_name, 'w') as f:
        f.write(marked_resume)

#main()
get_all_json_objects()
