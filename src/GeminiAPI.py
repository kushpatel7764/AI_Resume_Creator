import random
import json
import requests
from src.api_secrets import gemini_api_key
import sqlite3
# Name: Kush Patel
# Project Description: This program generates a markdown resume by combining a personal description with
#                      a randomly selected job description from rapid_jobs2.json. The markdown resume is
#                      generated by prompting the Gemini AI model to create a markdown resume based on this information.

# Note to self for github yaml command for pytest: python -m pytest tests/*
# TODO: Improve naming, a lot of variables that read something_name actually should be saying something_path


# Function to get all json objects
def get_all_json_objects(filenames):
    json_objects = []
    for filename in filenames:
        with open(filename, 'r') as f:
            for line in f:
                # Read each line in rapid_jobs2.json file as a JSON object
                objs_array_or_obj = json.loads(line)
                # If objs_array is an actual array then loop through it otherwise just add it to json_objects' list
                if type(objs_array_or_obj) is list:
                    for obj in objs_array_or_obj:
                        json_objects.append(obj)
                else:
                    json_objects.append(objs_array_or_obj)
    return json_objects


# Function to get a random job listing from the JSON file
def get_random_json_object():
    json_objects = get_all_json_objects(["../rapid_jobs2.json"])
    # Select a random job object
    random_job = random.choice(json_objects)
    return random_job


# Connect to database
def setup_job_database(cursor, conn):
    # Read the SQL file
    with open("../job_database.sql", "r") as file:
        sql_script = file.read()

    # Execute the SQL script (AI helped find the executescript() function)
    cursor.executescript(sql_script)
    # Commit changes
    conn.commit()


# Insert data into the job table
def insert_to_job(conn, cursor, job_info):
    job_id = job_info.get("id")

    # Check if job_id already exists
    cursor.execute("SELECT COUNT(*) FROM jobs WHERE id = ?", (job_id,))
    exists = cursor.fetchone()[0]

    if exists:
        print(f"Skipping job {job_id}, already exists.")
        return  # Skip inserting duplicate

    # There are two different date posted in the json files
    # There is date_posted and datePosted, this just fixes that issue
    date_posted = job_info.get("date_posted", None)
    date_posted2 = job_info.get("datePosted", None)
    date = ""
    if date_posted is not None:
        date = date_posted
    else:
        date = date_posted2

    cursor.execute("""
        Insert INTO jobs (id, site, job_url, job_url_direct, title, company_name, company_industry, company_url,
        company_url_direct, company_addresses, company_num_employees, company_revenue, company_description, logo_photo_url,
        banner_photo_url, ceo_name, ceo_photo_url, location, job_type, date_posted, salary_source, interval, min_amount,
        max_amount, currency, is_remote, job_level, job_function, listing_type, emails, description, employment_type,
        salary_range, image)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ? ,?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
    """, (job_info.get("id"),
        job_info.get("site", None),
        job_info.get("job_url", None),
        job_info.get("job_url_direct", None),
        job_info.get("title", None),
        job_info.get("company_name", None),
        job_info.get("company_industry", None),
        job_info.get("company_url", None),
        job_info.get("company_url_direct", None),
        job_info.get("company_addresses", None),  # Use company ID as foreign key
        job_info.get("company_num_employees", None),
        job_info.get("company_revenue", None),
        job_info.get("company_description", None),
        job_info.get("logo_photo_url", None),
        job_info.get("banner_photo_url", None),
        job_info.get("ceo_name", None),
        job_info.get("ceo_photo_url", None),
        job_info.get("location", None),
        job_info.get("job_type", None),
        date,
        job_info.get("salary_source", None),
        job_info.get("interval", None),
        job_info.get("min_amount", None),
        job_info.get("max_amount", None),
        job_info.get("currency", None),
        job_info.get("is_remote", None),
        job_info.get("job_level", None),
        job_info.get("job_function", None),
        job_info.get("listing_type", None),
        job_info.get("emails", None),
        job_info.get("description", None),
        job_info.get("employment_type", None),
        job_info.get("salary_range", None),
        job_info.get("image", None))
    )
    conn.commit()


# Function to insert into job providers table
def insert_to_job_provider(conn, cursor, job_id, providers):
    for provider in providers:
        provider_name = provider.get("jobProvider")

        # Check if job_id and provider_name already exist
        cursor.execute("SELECT COUNT(*) FROM job_providers WHERE job_id = ? AND provider_name = ?", (job_id, provider_name))
        exists = cursor.fetchone()[0]

        if exists:
            print(f"Skipping job {job_id} from provider {provider_name}, already exists.")
            return  # Skip inserting duplicat
        cursor.execute("""
            INSERT INTO job_providers (job_id, provider_name, provider_url)
            VALUES (?, ?, ?)
        """, (job_id, provider.get('jobProvider', None), provider.get('url', None)))
    conn.commit()


# Function creates the entire database and places jobs info from json files in the database as well.
def initialize_database(database_name, json_files):
    # Connect to the SQLite database (or create one if it doesn't exist)
    conn = sqlite3.connect(database_name)
    cursor = conn.cursor()
    setup_job_database(cursor, conn)
    all_json_obj = get_all_json_objects(json_files)
    for obj in all_json_obj:
        # Insert json info in database
        insert_to_job(conn, cursor, obj)
        insert_to_job_provider(conn, cursor, obj['id'], obj.get('jobProviders', []))
    print("Data successfully inserted!")
    conn.close()


def main():
    # Get the API key from the api_secrets file to access Google's Gemini AI model
    key = gemini_api_key
    # Using the random_json_object(), get a random job and then extract the job description from the job.
    job_json_random = get_random_json_object()
    job_description = job_json_random["description"]

    initialize_database("../Jobs_Database.db", ["../rapid_job1.json", "../rapid_jobs2.json"])

    # Personal information to be included in the resume
    personal_info = ("My name is Kush Patel. I am a computer science major studying at Bridgewater State University (BSU),"
                     " I am driven by a desire to innovate and problem solve. I am graduating from the BSU on May, 2025. "
                     "Some important courses I have completed at BSU are Web Application Development, Computer Networks, "
                     "Software Engineering, Cloud Computing, Introduction Database systems, Introduction to A.I., and "
                     "Unix/Linux System Admin.I was awarded Dr. Linda Wilkens and Dr. Glenn Pavlicek Scholarship at BSU in "
                     "recognition of my academic achievements such as a 4.0 major GPA. I am from East Greenwich, Road Island "
                     "but I currently live in Fairhaven, Massachusetts. Some programing languages that I am proficient at are"
                     " Python, Java, Swift, and JavaScript. I also MySQL from my database system course. Currently, I am "
                     "developing a bank statement processing application that efficiently converts bank statements into Excel"
                     " spreadsheets. With a positive attitude and a relentless motivation to learn, I'm eager to take on new "
                     "challenges and expand my expertise. Additionally, along with my current project, I am also doing a "
                     "research internship at Bridgewater State University in which I am developing a program that can "
                     "recognize a table in an image and convert it to an excel table. Some tools that I know are Git, "
                     "JetBrain IDEs, Xcode, and Vscode. My hobbies are playing basketball, cricket, and programing.")

    # Prepare the API request payload for Google's Gemini AI model
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={key}"
    headers = {'content-type': 'application/json'}
    data = {
        "contents": [{
            "parts": [{"text": f"Remember my Personal Information: {personal_info} \n Remember the Job description: "
                               f"{job_description} \n Now create a resume in markdown format that will be designed from "
                               f"my personal information, using keywords from job description that I provided"}]
        }]
    }

    # Send a post request to the Gemini API
    response = requests.post(url, headers=headers, json=data)
    response_json = response.json()

    # Extract the generated markdown resume from the API response
    marked_resume = response_json["candidates"][0]["content"]["parts"][0]["text"]

    # Save the generated resume to a markdown file
    new_file_name = "../Marked_Resume.md"
    with open(new_file_name, 'w') as f:
        f.write(marked_resume)

main()

